<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<!--                                             -->
<!-- Author: ROOT team (rootdev@hpsalo.cern.ch)  -->
<!--                                             -->
<!--   Date: Fri Dec 20 16:03:33 2002            -->
<!--                                             -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>TXMLP - source file</title>
<link rev=made href="mailto:rootdev@root.cern.ch">
<meta name="rating" content="General">
<meta name="objecttype" content="Manual">
<meta name="keywords" content="software development, oo, object oriented, unix, x11, windows, c++, html, rene brun, fons rademakers">
<meta name="description" content="ROOT - An Object Oriented Framework For Large Scale Data Analysis.">
</head>
<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#551a8b" ALINK="#ff0000" TEXT="#000000">
<a name="TopOfPage"></a>
<pre>
<b>//////////////////////////////////////////////////////////////////////////</b>
<b>//									//</b>
<b>// <a href=".././TXMLP.html">TXMLP</a>								//</b>
<b>//									//</b>
<b>// Implementation of the Multi-Layer-Perceptron				//</b>
<b>// Part of the Neural Network Objects package (NNO)			//</b>
<b>//									//</b>
<b>// Author List:								//</b>
<b>// Johannes Steffens, Bochum University					//</b>
<b>// M.Kunze, Bochum University						//</b>
<b>// (C) Copyright Johannes Steffens 1995, Ruhr-University Bochum.	//</b>
<b>//									//</b>
<b>//////////////////////////////////////////////////////////////////////////</b>

#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;stdarg.h&gt;
#include "RhoNNO/TXMLP.h"
#include "RhoNNO/VNeuralNetPlotter.h"

ClassImp(TXMLP)

#include &lt;iostream&gt;
using namespace std;

<a name="TXMLP:TXMLP"> </a>TXMLP:: TXMLP(Int_t layers,Double_t inputRange,const char* netFile,Int_t innodes,...)
: VSupervisedNet("XMLP",innodes,0,netFile) 
{    
    Int_t I;
    if (layers&lt;1) Errorf("(TXMLP) at least one layer neccessary");
    fParm.fLayers  = layers;
    fParm.fInScale = 1.0/inputRange;
    
    Int_t*    nodes = new Int_t   [fParm.fLayers];      TestPointer(nodes);
    Double_t* step  = new Double_t[fParm.fLayers];      TestPointer(step);
    TNeuralNetParameters::TRANSFER* func  = new TNeuralNetParameters::TRANSFER[fParm.fLayers];      TestPointer(func);
    fPerc           = new TPerceptron*[fParm.fLayers]; TestPointer(fPerc);
    
    va_list ap;
    va_start(ap,innodes);
    
    for (I=0;I&lt;fParm.fLayers;++I) nodes[I] = va_arg(ap,Int_t);
    for (I=0;I&lt;fParm.fLayers;++I) step[I]  = va_arg(ap,Double_t);
    for (I=0;I&lt;fParm.fLayers;++I) func[I]  = (TNeuralNetParameters::TRANSFER) va_arg(ap,Int_t);
    
    va_end(ap);
    
    fParm.fOutNodes = nodes[layers-1];
    
    for (I=0;I&lt;fParm.fLayers;++I) {
	if (I==0)
	    fPerc[I] = new TPerceptron(fParm.fInNodes,nodes[I],step[I],func[I],I);
	else
	    fPerc[I] = new TPerceptron(fPerc[I-1],nodes[I],step[I],func[I],I);

	TestPointer(fPerc[I]);
    }
    fOut = new Double_t[fParm.fOutNodes]; // As we did not know before, allocate here...
    TestPointer(fOut);
    
    delete[] nodes;
    delete[] step;
    delete[] func;
}

<a name="TXMLP:TXMLP"> </a>TXMLP:: TXMLP(Int_t layers,Double_t inputRange,const char* netFile,Int_t innodes,Int_t n0,Int_t n1,Int_t n2,Double_t s0,Double_t s1,Double_t s2,
	      TNeuralNetParameters::TRANSFER f0,TNeuralNetParameters::TRANSFER f1,TNeuralNetParameters::TRANSFER f2)
: VSupervisedNet("XMLP",innodes,0,netFile) 
{    
    Int_t I;
    if (layers!=3) Errorf("(TXMLP) Constructor needs 3 Layers");
    fParm.fLayers  = layers;
    fParm.fInScale = 1.0/inputRange;
    
    Int_t*    nodes = new Int_t   [fParm.fLayers];      TestPointer(nodes);
    Double_t* step  = new Double_t[fParm.fLayers];      TestPointer(step);
    TNeuralNetParameters::TRANSFER* func  = new TNeuralNetParameters::TRANSFER[fParm.fLayers];      TestPointer(func);
    fPerc           = new TPerceptron*[fParm.fLayers];  TestPointer(fPerc);
    
    nodes[0] = n0;nodes[1] = n1;nodes[2] = n2;
    step[0]  = s0;step[1]  = s1;step[2]  = s2;
    func[0]  = f0;func[1]  = f1;func[2]  = f2;
    
    fParm.fOutNodes = nodes[layers-1];
    
    for (I=0;I&lt;fParm.fLayers;++I) {
	if (I==0)
	    fPerc[I] = new TPerceptron(fParm.fInNodes,nodes[I],step[I],func[I],I);
	else
	    fPerc[I] = new TPerceptron(fPerc[I-1],nodes[I],step[I],func[I],I);
	
	TestPointer(fPerc[I]);
    }
    fOut = new Double_t[fParm.fOutNodes]; // As we did not know before, allocate here...
    TestPointer(fOut);
    
    delete[] nodes;
    delete[] step;
    delete[] func;
}

<a name="TXMLP:AllocNet"> </a>void TXMLP:: AllocNet(void) 
{
    fPerc = new TPerceptron*[fParm.fLayers]; TestPointer(fPerc); // MK: Allocation
    Int_t I;
    for (I=0;I&lt;fParm.fLayers;++I) {
	if (I==0)
	    fPerc[I] = new TPerceptron();
	else
	    fPerc[I] = new TPerceptron(fPerc[I-1]);
	
	TestPointer(fPerc[I]);
    }
}

<a name="TXMLP:~TXMLP"> </a>TXMLP::~TXMLP() 
{
    if (fFilename!="") if (fShouldSave) Save();
    Int_t I;
    for (I=0;I&lt;fParm.fLayers;++I) delete fPerc[I];
    delete[] fPerc;
}

<a name="TXMLP:ReadBinary"> </a>void TXMLP::ReadBinary(void) 
{
    Int_t I;
    fread(&amp;fParm,sizeof(TNeuralNetParameters),1,fFile);
    AllocNet();
    for (I=0;I&lt;fParm.fLayers;++I) {
	fPerc[I]-&gt;SetFile(fFile);
	fPerc[I]-&gt;ReadBinary();
    }
}

<a name="TXMLP:ReadText"> </a>void  TXMLP::ReadText(void) 
{
    Int_t I;
    Int_t layers;
    Double_t scale;
    fscanf(fFile,"layers    %i\n",&amp;layers);
    fParm.fLayers = layers;
    fscanf(fFile,"in_scale  %le\n",&amp;scale);
    fParm.fInScale = scale;
    AllocNet();
    for (I=0;I&lt;fParm.fLayers;++I) {
	fPerc[I]-&gt;SetFile(fFile);
	fPerc[I]-&gt;ReadText();
    }
}

<a name="TXMLP:WriteBinary"> </a>void TXMLP::WriteBinary(void) 
{
    Int_t I;
    fwrite(&amp;fParm,sizeof(TNeuralNetParameters),1,fFile);
    for (I=0;I&lt;fParm.fLayers;++I) {
	fPerc[I]-&gt;SetFile(fFile);
	fPerc[I]-&gt;WriteBinary();
    }
}

<a name="TXMLP:WriteText"> </a>void  TXMLP::WriteText(void) 
{
    Int_t I;
    fprintf(fFile,"layers    %i\n",fParm.fLayers);
    fprintf(fFile,"in_scale  %le\n",fParm.fInScale);
    for (I=0;I&lt;fParm.fLayers;++I) {
	fPerc[I]-&gt;SetFile(fFile);
	fPerc[I]-&gt;WriteText();
    }
}


<a name="TXMLP:Recall"> </a>Double_t* TXMLP::Recall(NNO_INTYPE* in,NNO_OUTTYPE* out) 
{
    Int_t I;
    
<b>    // convert input</b>
    <a href="../ListOfTypes.html#NNO_INTYPE">NNO_INTYPE</a>* i=in;
    <a href="../ListOfTypes.html#Double_t">Double_t</a>* pi = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[0]-&gt;fIn;
    for (I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fInNodes;++I) *pi++ = *i++ * <a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fInScale;
    
<b>    // recallstep of each perceptron</b>
    for (I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fLayers;++I) <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[I]-&gt;<a href=".././TPerceptron.html#TPerceptron:Recall">Recall</a>();
    for (I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fOutNodes;++I) <a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[I] = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[fParm.fLayers-1]-&gt;<a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[I];

    if (<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>) {
	<a href="../ListOfTypes.html#Bool_t">Bool_t</a> good = kTRUE;
	if (out!=0) good = out[0]&gt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fThreshold;
	<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>-&gt;<a href=".././VNeuralNetPlotter.html#VNeuralNetPlotter:AddTestSample">AddTestSample</a>(<a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[0],good);
    }

    return <a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>;
}

<a name="TXMLP:Train"> </a><a href="../ListOfTypes.html#Double_t">Double_t</a> <a href=".././TXMLP.html#TXMLP:Train">TXMLP::Train</a>(<a href="../ListOfTypes.html#NNO_INTYPE">NNO_INTYPE</a>* in,<a href="../ListOfTypes.html#NNO_OUTTYPE">NNO_OUTTYPE</a>* trout) 
{
    <a href="../ListOfTypes.html#Int_t">Int_t</a> I,J;
    <a href=".././VNeuralNet.html#VNeuralNet:fShouldSave">fShouldSave</a> = kTRUE;

<b>    // convert input</b>
    <a href="../ListOfTypes.html#NNO_INTYPE">NNO_INTYPE</a>* i=in;
    <a href="../ListOfTypes.html#Double_t">Double_t</a>* pi = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[0]-&gt;fIn;
    for (I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fInNodes;++I) *pi++ = *i++ * <a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fInScale;
    
<b>    // recallstep of each perceptron</b>
    for (I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fLayers;++I) <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[I]-&gt;<a href=".././TPerceptron.html#TPerceptron:Recall">Recall</a>();
    for (I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fOutNodes;++I) <a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[I] = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[fParm.fLayers-1]-&gt;<a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[I];
    
    <a href="../ListOfTypes.html#Double_t">Double_t</a> S_Err = 0;
    <a href="../ListOfTypes.html#Double_t">Double_t</a>*   d = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[fParm.fLayers-1]-&gt;fDiffSrc;
    <a href="../ListOfTypes.html#Double_t">Double_t</a>* out = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[fParm.fLayers-1]-&gt;<a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>;
    <a href="../ListOfTypes.html#NNO_OUTTYPE">NNO_OUTTYPE</a>* tr_out=trout;
    for (J=0;J&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fOutNodes;++J) { 
	*d = *tr_out++ - *out++; 
	S_Err += *d * *d; 
	d++; 
    }

    for (I=<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fLayers-1;I&gt;=0;--I) <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[I]-&gt;<a href=".././TPerceptron.html#TPerceptron:Train">Train</a>();

    if (<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>) <a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>-&gt;<a href=".././VNeuralNetPlotter.html#VNeuralNetPlotter:AddTrainSample">AddTrainSample</a>(trout[0],trout[0]&gt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fThreshold);

    return S_Err;
}

<a name="TXMLP:SetMomentumTerm"> </a><a href="../ListOfTypes.html#void">void</a> <a href=".././TXMLP.html#TXMLP:SetMomentumTerm">TXMLP::SetMomentumTerm</a>(<a href="../ListOfTypes.html#Double_t">Double_t</a> f)
{
    for (<a href="../ListOfTypes.html#int">int</a> I=0;I&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fLayers;++I) {
	<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fMu = f;
	<a href=".././TNeuralNetParameters.html">TNeuralNetParameters</a> &amp;parm = <a href=".././TXMLP.html#TXMLP:fPerc">fPerc</a>[I]-&gt;GetParameters();
	parm.fMu = f;
    }
}
</pre>

<!--SIGNATURE-->
<br>
<hr>
<center>
<address>
<a href="http://root.cern.ch/root/Welcome.html">ROOT page</a> - <a href="../ClassIndex.html">Class index</a> - <a href="#TopOfPage">Top of the page</a><br>
</address>
</center>
<hr>
<address>
This page has been automatically generated. If you have any comments or suggestions about the page layout send a mail to <a href="mailto:rootdev@root.cern.ch">ROOT support</a>, or contact <a href="mailto:rootdev@root.cern.ch">the developers</a> with any questions or problems regarding ROOT.
</address>
</body>
</html>
