<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<!--                                             -->
<!-- Author: ROOT team (rootdev@hpsalo.cern.ch)  -->
<!--                                             -->
<!--   Date: Fri Dec 20 16:03:06 2002            -->
<!--                                             -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>TNNK - source file</title>
<link rev=made href="mailto:rootdev@root.cern.ch">
<meta name="rating" content="General">
<meta name="objecttype" content="Manual">
<meta name="keywords" content="software development, oo, object oriented, unix, x11, windows, c++, html, rene brun, fons rademakers">
<meta name="description" content="ROOT - An Object Oriented Framework For Large Scale Data Analysis.">
</head>
<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#551a8b" ALINK="#ff0000" TEXT="#000000">
<a name="TopOfPage"></a>
<pre>
<b>//////////////////////////////////////////////////////////////////////////</b>
<b>//									//</b>
<b>// <a href=".././TNNK.html">TNNK</a>									//</b>
<b>//									//</b>
<b>// Interface to J.P Ernenwein's neural network kernel <a href=".././TNNKernel.html">TNNKernel</a>		//</b>
<b>// Part of the Neural Network Objects package (NNO)			//</b>
<b>//									//</b>
<b>// Author List:								//</b>
<b>// M.Kunze, Bochum University						//</b>
<b>// (C) Copyright 2001, Ruhr-University Bochum.				//</b>
<b>//									//</b>
<b>//////////////////////////////////////////////////////////////////////////</b>

#include "RhoNNO/TNNK.h"
#include "RhoNNO/VNeuralNetPlotter.h"

ClassImp(TNNK)

#include &lt;iostream&gt;
using namespace std;

<a name="TNNK:AllocNet"> </a>void TNNK::AllocNet(void)
{}

<a name="TNNK:InitNet"> </a>void TNNK::InitNet(void)
{}

<a name="TNNK:WriteText"> </a>void TNNK::WriteText(void)
{
    if (fFile!=0) fclose(fFile);
    fKernel-&gt;Export((char*)fFilename.Data());
}

<a name="TNNK:WriteBinary"> </a>void TNNK::WriteBinary(void)
{}

<a name="TNNK:ReadText"> </a>void TNNK::ReadText(void)
{
    if (fFile!=0) fclose(fFile);
    fKernel-&gt;Import((char*)fFilename.Data());
}

<a name="TNNK:ReadBinary"> </a>void TNNK::ReadBinary(void)
{}

<a name="TNNK:TNNK"> </a>TNNK::TNNK(Double_t learn,Double_t fse,Double_t mu,Int_t innodes,Text_t *hidnodes,Int_t outnodes,const char* netFile)
: VSupervisedNet("TNNK",innodes,outnodes,netFile) 
{
    fShouldSave = kTRUE;
    Text_t *name = "TNNK";
    fKernel = new TNNKernel(name,innodes,hidnodes,outnodes);
    fKernel-&gt;SetLearnParam(learn,fse,mu);
    fKernel-&gt;SetArraySize(1);
    fKernel-&gt;SetInitParam(-2.,2.);
    fKernel-&gt;SetUseBiases();
    fKernel-&gt;Init();
    fKernel-&gt;PrintS();
}

<a name="TNNK:TNNK"> </a>TNNK::TNNK(const char* netFile)
: VSupervisedNet("TNNK",5,10,netFile)
{
    fShouldSave = kFALSE;
    fKernel = new TNNKernel();
    ReadText();
}

<a name="TNNK:~TNNK"> </a>TNNK::~TNNK()
{
    if (fShouldSave) WriteText();
    delete fKernel;
}

<a name="TNNK:Train"> </a>Double_t TNNK::Train(NNO_INTYPE* in,NNO_OUTTYPE* out)
{
    int i;

    for (i=0;i&lt;fParm.fOutNodes;i++) {
	fKernel-&gt;SetTeach(out[i],i);
    }

    for (i=0;i&lt;fParm.fInNodes;i++) {
	fKernel-&gt;SetInput(in[i],i);
    }

    fKernel-&gt;Forward();
    Double_t error = fKernel-&gt;Error();
    fKernel-&gt;LearnBackward();

<b>    // Copy over to base class</b>
    for (i=0;i&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fOutNodes;i++) {
	<a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[i] = <a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:GetOutput">GetOutput</a>(i);
    }

    if (<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>) {
	<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>-&gt;<a href=".././VNeuralNetPlotter.html#VNeuralNetPlotter:AddTrainSample">AddTrainSample</a>(out[0],out[0]&gt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fThreshold);
    }

    return error;
}

<a name="TNNK:Recall"> </a><a href="../ListOfTypes.html#Double_t">Double_t</a>* <a href=".././TNNK.html#TNNK:Recall">TNNK::Recall</a>(<a href="../ListOfTypes.html#NNO_INTYPE">NNO_INTYPE</a>* in,<a href="../ListOfTypes.html#NNO_OUTTYPE">NNO_OUTTYPE</a>* out)
{
    <a href="../ListOfTypes.html#int">int</a> i;

    for (i=0;i&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fOutNodes;i++) {
	<a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:SetTeach">SetTeach</a>(out[i],i);
    }

    for (i=0;i&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fInNodes;i++) {
	<a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:SetInput">SetInput</a>(in[i],i);
    }

    <a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:GoThrough">GoThrough</a>();
    <a href="../ListOfTypes.html#Double_t">Double_t</a> error = <a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:Error">Error</a>();

<b>    // Copy over to base class</b>
    for (i=0;i&lt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fOutNodes;i++) {
	<a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>[i] = <a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:GetOutput">GetOutput</a>(i);
    }

    if (<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>) {
	<a href="../ListOfTypes.html#Bool_t">Bool_t</a> good = kTRUE;
	if (out!=0) good = out[0]&gt;<a href=".././VNeuralNet.html#VNeuralNet:fParm">fParm</a>.fThreshold;
	<a href=".././VNeuralNet.html#VNeuralNet:fPlotter">fPlotter</a>-&gt;<a href=".././VNeuralNetPlotter.html#VNeuralNetPlotter:AddTestSample">AddTestSample</a>(<a href=".././TNNK.html#TNNK:fKernel">fKernel</a>-&gt;<a href=".././TNNKernel.html#TNNKernel:GetOutput">GetOutput</a>(),good);
    }

    return <a href=".././VNeuralNet.html#VNeuralNet:fOut">fOut</a>;
}

<b>//////////////////////////////////////////////////////////////////</b>
<b>//</b>
<b>//  <a href=".././TNNKernel.html">TNNKernel</a> </b>
<b>//  Feed-Forward Neural Network </b>
<b>//</b>
<b>//////////////////////////////////////////////////////////////////</b>

#include "TDatime.h"

ClassImp(TNNKernel)

TNNKernel::TNNKernel(Text_t *name, Int_t nInput, Text_t *hidden, Int_t nOutput):TNamed(name,"Neural Network"),
 fW(0), fNUnits(0), fArrayIn(0), fArrayOut(0), fEventsList(0), fValues(0), fTeach(0)
{
<b>    // constructor</b>
    AllocateVW(nInput,hidden,nOutput);
    
    fUseBiases=1.;
    fLearnParam=0.2;
    fFlatSE=0.;
    fMu=0.;
    fLowerInitWeight=-1.;
    fUpperInitWeight=1.;
    
    fNTrainEvents=0;
    
    fNTrainCycles=0;
    
    TDatime temps;
    fRandom.SetSeed(temps.Convert());
    printf("First Random Seed = %in",fRandom.GetSeed());
    printf("Neural Network is created : n");
<b>    //  PrintS();</b>
    
}

<a href=".././TNNKernel.html#TNNKernel:TNNKernel">TNNKernel::TNNKernel</a>() : fW(0), fNUnits(0), fArrayIn(0), fArrayOut(0), 
                         fEventsList(0), fValues(0), fTeach(0)
{
<b>    // constructor witn no parameter </b>
    fUseBiases=1.;
    fLearnParam=0.2;
    fFlatSE=0.;
    fMu=0.;
    fLowerInitWeight=-1.;
    fUpperInitWeight=1.;
    fNHiddL=0;
    
    fNTrainEvents=0;
    
    fNTrainCycles=0;
    
    TDatime temps;
    fRandom.SetSeed(temps.Convert());
    printf("First Random Seed = %in",fRandom.GetSeed());
}


<a href=".././TNNKernel.html">TNNKernel</a>::~<a href=".././TNNKernel.html">TNNKernel</a>() 
{
<b>    // destructor  </b>
    
    DeleteArray();
    FreeVW();
    if(fEventsList) delete [] fEventsList;
}  


<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetHidden">TNNKernel::SetHidden</a>(<a href="../ListOfTypes.html#Text_t">Text_t</a> *ttext)
{
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,j;
    <a href="../ListOfTypes.html#Text_t">Text_t</a> text[100];
    strcpy(text,ttext);
    
    fNHiddL=1;
    for (i=0;text[i];i++)if(text[i]==':')fNHiddL++;
    if (fNUnits) delete [] fNUnits;
    fNUnits = new <a href="../ListOfTypes.html#Int_t">Int_t</a>[fNHiddL+2];
    
    j=0;
    for (i=1;i&lt;=fNHiddL;i++)
    {
	TString number;
	while(text[j]&amp;&amp;(text[j]!=':')){number.Append(text[j]);j++;}
	j++;
	sscanf(number.Data(),"%i",&amp;fNUnits[i]);  
	printf("%i n",fNUnits[i]); 
    }
    
}


<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:FreeVW">TNNKernel::FreeVW</a>()
{
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l;
    
<b>    // free of values</b>
    
    if (fValues)
    {
	for (i=0;i&lt;fNHiddL+2;i++)
	{delete [] fValues[i]; delete [] fErrors[i]; delete [] fBiases[i];delete [] fDB[i];} 
	delete [] fValues; delete [] fErrors; delete [] fBiases;delete [] fDB;
	fValues=0;
    }
    
<b>    // free of teaching</b>
    
    if (fTeach) 
    {
	delete [] fTeach;
	fTeach=0;
    }
    
<b>    // free of weights</b>
    
    if (fW)
    {
	for (i=0;i&lt;fNHiddL+1;i++)
	{
	    for(l=0;l&lt;fNUnits[i];l++){delete [] fW[i][l];delete [] fDW[i][l];}  
	    delete [] fW[i];delete [] fDW[i];
	}    
	fW=0;
    }
    
<b>    // free of units</b>
    
    if (fNUnits){ delete [] fNUnits; fNUnits=0;}
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:AllocateVW">TNNKernel::AllocateVW</a>(<a href="../ListOfTypes.html#Int_t">Int_t</a> nInput, <a href="../ListOfTypes.html#Text_t">Text_t</a> *hidden, <a href="../ListOfTypes.html#Int_t">Int_t</a> nOutput)
{
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l;
    
    if(fW){printf("free memory first !n");return;}
    
    SetHidden(hidden);
    fNUnits[0]=nInput;
    fNUnits[fNHiddL+1]=nOutput;
    
<b>    // allocation of values</b>
    
    fValues = new <a href="../ListOfTypes.html#Float_t">Float_t</a>*[fNHiddL+2];
    fErrors = new <a href="../ListOfTypes.html#Double_t">Double_t</a>*[fNHiddL+2];
    fBiases = new <a href="../ListOfTypes.html#Double_t">Double_t</a>*[fNHiddL+2];
    fDB = new <a href="../ListOfTypes.html#Double_t">Double_t</a>*[fNHiddL+2];
    
    for (i=0;i&lt;fNHiddL+2;i++)
    {
	fValues[i]=new <a href="../ListOfTypes.html#Float_t">Float_t</a>[fNUnits[i]]; 
	fErrors[i]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>[fNUnits[i]]; 
	fBiases[i]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>[fNUnits[i]]; 
	fDB[i]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>[fNUnits[i]]; 
	for (<a href="../ListOfTypes.html#int">int</a> j=0;j&lt;fNUnits[i];j++) {
	    fValues[i][j] = 0.0;
	    fErrors[i][j] = 0.0;
	    fBiases[i][j] = 0.0;
	    fDB[i][j] = 0.0;
	}
    }
    
<b>    // allocation of teaching</b>
    
    fTeach=new <a href="../ListOfTypes.html#Float_t">Float_t</a>[fNUnits[fNHiddL+1]]; 
    
<b>    // allocation of weights</b>
    
    fW=new <a href="../ListOfTypes.html#Double_t">Double_t</a>**[fNHiddL+1];
    fDW=new <a href="../ListOfTypes.html#Double_t">Double_t</a>**[fNHiddL+1];
    
    for (i=0;i&lt;fNHiddL+1;i++)
    {
	fW[i]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>*[fNUnits[i]];
	fDW[i]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>*[fNUnits[i]];
	for (l=0;l&lt;fNUnits[i];l++)
	{
	    fW[i][l]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>[fNUnits[i+1]];  
	    fDW[i][l]=new <a href="../ListOfTypes.html#Double_t">Double_t</a>[fNUnits[i+1]]; 
	}
    }
    
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetKernel">TNNKernel::SetKernel</a>(<a href="../ListOfTypes.html#Int_t">Int_t</a> nInput, <a href="../ListOfTypes.html#Text_t">Text_t</a> *hidden, <a href="../ListOfTypes.html#Int_t">Int_t</a> nOutput)
{  
    FreeVW();
    AllocateVW(nInput,hidden,nOutput);
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetLearnParam">TNNKernel::SetLearnParam</a>(<a href="../ListOfTypes.html#Double_t">Double_t</a> learnParam,<a href="../ListOfTypes.html#Double_t">Double_t</a> fse,<a href="../ListOfTypes.html#Double_t">Double_t</a> mu)
{
<b>    // Sets the learning parameters :</b>
<b>    // the main learning parameter is around 0.2 (in ]0,1])</b>
<b>    // fse is for flat spot elimination, with values in [0,0.25], often 0.1</b>
<b>    // mu is for backprop momentum, values in [0,1]</b>
    fLearnParam=TMath::Abs(learnParam);
    fFlatSE=TMath::Abs(fse);
    fMu=TMath::Abs(mu);
    
    if (fLearnParam&gt;1.0) printf("Warning : %6.2f is not an usual valuen",fLearnParam);
    if (fLearnParam==0.0) printf("Warning : 0 is a stupid valuen");
    printf("Learning Parameter set to : %6.2fn",fLearnParam);
    printf("Flat Spot elimination value  set to : %6.2fn",fFlatSE);
    printf("Momentum set to : %6.2fn",fMu);
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetInitParam">TNNKernel::SetInitParam</a>(<a href="../ListOfTypes.html#Float_t">Float_t</a> lowerInitWeight, <a href="../ListOfTypes.html#Float_t">Float_t</a> upperInitWeight)
{
<b>    // Sets the initialisation parameters : max and min weights </b>
    <a href="../ListOfTypes.html#Float_t">Float_t</a> temp;
    
    fLowerInitWeight=lowerInitWeight;
    fUpperInitWeight=upperInitWeight;
    if (fLowerInitWeight&gt;fUpperInitWeight)
    {
	temp=fUpperInitWeight;
	fUpperInitWeight=fLowerInitWeight;
	fLowerInitWeight=temp;
    } 
    if (fLowerInitWeight==fUpperInitWeight)printf("Warning : the weights initialisation bounds are equal !n");
    printf("Init Parameters set to :n");
    printf(" --&gt; Lower bound = %6.2fn",fLowerInitWeight);
    printf(" --&gt; Upper bound = %6.2fn",fUpperInitWeight);
    
}


<a href="../ListOfTypes.html#Float_t">Float_t</a> <a href=".././TNNKernel.html#TNNKernel:Alea">TNNKernel::Alea</a>()
{
    return (<a href="../ListOfTypes.html#Float_t">Float_t</a>) fLowerInitWeight+fRandom.Rndm()*(fUpperInitWeight-fLowerInitWeight);
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:Init">TNNKernel::Init</a>()
{
<b>    // initialisation of  biases and weights.  </b>
<b>    // the init parameters can be changed by :</b>
<b>    // SetInitParam(<a href="../ListOfTypes.html#Float_t">Float_t</a> lowerInitWeight, <a href="../ListOfTypes.html#Float_t">Float_t</a> upperInitWeight)</b>
<b>    // The default is -1 and 1</b>
    
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c;
    
    if(!fW){printf("allocate memory first !n");return;}
    
<b>    // init of weights</b>
    
    for (i=0;i&lt;fNHiddL+1;i++)
	for (l=0;l&lt;fNUnits[i];l++)
	    for (c=0;c&lt;fNUnits[i+1];c++) fW[i][l][c]=(<a href="../ListOfTypes.html#Double_t">Double_t</a>)Alea();
	    
    for(i=0;i&lt;fNHiddL+1;i++)for(l=0;l&lt;fNUnits[i];l++)for(c=0;c&lt;fNUnits[i+1];c++)
	fDW[i][l][c]=0.;       
    
<b>    // init of biases</b>
    
    for (i=1;i&lt;fNHiddL+2;i++)
	for (l=0;l&lt;fNUnits[i];l++) fBiases[i][l]=(<a href="../ListOfTypes.html#Double_t">Double_t</a>)(Alea())*fUseBiases;
	
    for(i=1;i&lt;fNHiddL+2;i++)for(l=0;l&lt;fNUnits[i];l++)fDB[i][l]=0.;
    
    
    fNTrainCycles=0;
    printf("Initialisation donen");
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:PrintS">TNNKernel::PrintS</a>()
{
<b>    // prints structure of network on screen</b>
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c;
    
    if(!fW){printf("no unit !n");return;} 
    
    printf("+++++++++ Neural Network %s ++++++++++++n",GetName());
    for(i=0;i&lt;fNHiddL+2;i++)printf("Layer %1i contains %2i unitsn",i,fNUnits[i]);
    
    if(fUseBiases)printf("&gt;&gt;&gt;&gt;&gt;&gt;&gt; Biases USED");else printf("&gt;&gt;&gt;&gt;&gt;&gt;&gt;Biases DUMMY");
    
    printf("n ----------   Biases   ---------- n");
    <a href="../ListOfTypes.html#Int_t">Int_t</a> maxl=0;
    for(i=0;i&lt;fNHiddL+2;i++)if(fNUnits[i]&gt;=maxl)maxl=fNUnits[i];
    for(i=0;i&lt;fNHiddL+2;i++)printf("    %1i   | ",i);printf("n");
    for(i=0;i&lt;fNHiddL+2;i++)printf("--------|-");printf("n");
    for(l=0;l&lt;maxl;l++)
    {
	for(i=0;i&lt;fNHiddL+2;i++)
	    if(l&lt;fNUnits[i])printf("%6.2f  | ",fBiases[i][l]);else printf("        | ");
	    printf("n");
    }
    
    
    printf("n    ----------   Weights ----------- n");
    for(i=0;i&lt;fNHiddL+1;i++)
    {
	printf(" From  %1i  to  %1i  : n",i,i+1);
	printf("%2i |",i);for(l=0;l&lt;fNUnits[i];l++)printf("  %3i |",l);printf("n");
	printf("===|");for(l=0;l&lt;fNUnits[i];l++)printf("-------");printf("n");
	printf("%2i |",i+1);for(l=0;l&lt;fNUnits[i];l++)printf("-------");printf("n");
	for(c=0;c&lt;fNUnits[i+1];c++)
	{ 
	    printf("%2i |",c);
	    for(l=0;l&lt;fNUnits[i];l++)printf("%6.2f|",fW[i][l][c]);
	    printf("n");
	}     
	printf("n");
    }  
    
    printf("n");
    printf("Learning parameter = %6.2fn",fLearnParam);
    printf("Flat Spot elimination value = %6.2fn",fFlatSE);
    printf("Momentum = %6.2fn",fMu);
    printf("Lower initialisation weight = %6.2fn",fLowerInitWeight);
    printf("Upper initialisation weight = %6.2fn",fUpperInitWeight);
    printf("Number of events for training   = %5in",fNTrainEvents);
    printf("Number of events for validation = %5in",fNValidEvents);
    printf("Number of cycles done = %3in",fNTrainCycles);
    printf("+++++++++++++++++++++++++++++++++++++++++++++++n");
    
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:Forward">TNNKernel::Forward</a>()
{
<b>    // general function to propagate the input activation </b>
<b>    //  The input activation array must be filled  </b>
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c;
    <a href="../ListOfTypes.html#Double_t">Double_t</a> sum;
    
    if(!fW){printf("no unit !n");return;}  
    
    for (i=0;i&lt;fNHiddL+1;i++)  
	for (c=0;c&lt;fNUnits[i+1];c++)
	{
	    sum=0.; 
	    for(l=0;l&lt;fNUnits[i];l++)sum+=fW[i][l][c]*(<a href="../ListOfTypes.html#Double_t">Double_t</a>)fValues[i][l];
	    fValues[i+1][c]=(<a href="../ListOfTypes.html#Float_t">Float_t</a>)Sigmoide(sum+fBiases[i+1][c]*fUseBiases);
	}
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:LearnBackward">TNNKernel::LearnBackward</a>()
{
<b>    // gradient retropropagation (updates of biases and weights)  </b>
    
    if(fNTrainEvents&lt;1){printf("No event to train !!!n");return;}
    if(!fW){printf("no unit !n");return;}
    
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c;
    <a href="../ListOfTypes.html#Double_t">Double_t</a> delta;
    
<b>    // weights</b>
    
    for (i=0;i&lt;fNHiddL+1;i++)  
	for (l=0;l&lt;fNUnits[i];l++)
	    for(c=0;c&lt;fNUnits[i+1];c++)
	    {
		delta=fLearnParam*fErrors[i+1][c]*(<a href="../ListOfTypes.html#Double_t">Double_t</a>)fValues[i][l]+fMu*fDW[i][l][c];
		fW[i][l][c]+=delta;
		fDW[i][l][c]=delta;
	    }

<b>    // biases</b>
    if(((<a href="../ListOfTypes.html#Bool_t">Bool_t</a>)fUseBiases))
    {
	for (i=1;i&lt;fNHiddL+2;i++)  
	    for (l=0;l&lt;fNUnits[i];l++)
	    {
		delta=fLearnParam*fErrors[i][l]+fMu*fDB[i][l];
		fBiases[i][l]+=delta;
		fDB[i][l]=delta;
	    }
    }
}

<a href="../ListOfTypes.html#Double_t">Double_t</a> <a href=".././TNNKernel.html#TNNKernel:Error">TNNKernel::Error</a>()
{
<b>    // function to compute the errors between forward propagation and teaching.  </b>
<b>    // this error is = |teaching-computed| summed on NN outputs and divided by their number.  </b>
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c;
    <a href="../ListOfTypes.html#Double_t">Double_t</a> sum,error=0,errorOneUnit;
    if(!fW){printf("no unit !n");return 0;}    
    
<b>    //  Error on Output Units</b>
    
    for(l=0;l&lt;fNUnits[fNHiddL+1];l++)
    {
	errorOneUnit=(<a href="../ListOfTypes.html#Double_t">Double_t</a>)(fTeach[l]-fValues[fNHiddL+1][l]);
	error+=TMath::Abs(errorOneUnit);
	fErrors[fNHiddL+1][l]=errorOneUnit*(SigPrim(fValues[fNHiddL+1][l])+fFlatSE);
    }
    error=error/(<a href="../ListOfTypes.html#Double_t">Double_t</a>)fNUnits[fNHiddL+1];
    
<b>    //  Error on Hidden Units</b>
    
    for(i=fNHiddL;i==1;i--)
    {  
	for(l=0;l&lt;fNUnits[i];l++)
	{
	    sum=0.;
	    for(c=0;c&lt;fNUnits[i+1];c++) sum+=fW[i][l][c]*fErrors[i+1][c];
	    fErrors[i][l]=sum*(SigPrim((<a href="../ListOfTypes.html#Double_t">Double_t</a>)fValues[i][l])+fFlatSE);
	}  
    }
    
    return error;
}

<a href="../ListOfTypes.html#Double_t">Double_t</a> <a href=".././TNNKernel.html#TNNKernel:ErrorO">TNNKernel::ErrorO</a>()
{
<b>    // function to compute the errors between forward propagation and teaching.  </b>
<b>    // this error is = |teaching-computed| summed on NN outputs and divided by their number.  </b>
<b>    //  Error on Output Units</b>
    
    <a href="../ListOfTypes.html#Int_t">Int_t</a> l;
    <a href="../ListOfTypes.html#Double_t">Double_t</a> error=0;
    if(!fW){printf("no unit !n");return 0;}    
    for(l=0;l&lt;fNUnits[fNHiddL+1];l++)
	error+=TMath::Abs((<a href="../ListOfTypes.html#Double_t">Double_t</a>)(fTeach[l]-fValues[fNHiddL+1][l]));
    
    error=error/(<a href="../ListOfTypes.html#Double_t">Double_t</a>)fNUnits[fNHiddL+1];  
    
    return error;
    
}  

<a href="../ListOfTypes.html#Double_t">Double_t</a> <a href=".././TNNKernel.html#TNNKernel:TrainOneCycle">TNNKernel::TrainOneCycle</a>()
{
<b>    // one loop on internal events = one cycle.  </b>
<b>    // takes each event from internal array in an order fixed by an array ( fEventsList ).</b>
<b>    // It is necessary to call the method Mix() before each call to this function</b>
<b>    // in order to change the presentation order.</b>
<b>    // The learning is done by this function.</b>
<b>    // The private variable  fNTrainCycles is incremented.</b>
    
    if(fNTrainEvents&lt;1){printf("No event to train !!!n");return 0.;}
    if(!fW){printf("no unit !n");return 0.;}
    
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i;
    <a href="../ListOfTypes.html#Double_t">Double_t</a> error=0.;
    
    for(i=0;i&lt;fNTrainEvents;i++)
    {  
	GetArrayEvt(fEventsList[i]); 
	Forward();
	error+=Error();
	LearnBackward();
    }
    
    fNTrainCycles++;
    error=error/(<a href="../ListOfTypes.html#Double_t">Double_t</a>)fNTrainEvents;
<b>    //  printf("cycle %i : E_t = %6.4f ",fNTrainCycles,error);</b>
    
    return error;
}
/*
<a href="../ListOfTypes.html#Double_t">Double_t</a> <a href=".././TNNKernel.html#TNNKernel:Valid">TNNKernel::Valid</a>()
{
<b>    // one loop on valid events.  </b>
<b>    // takes each event from validation tree.</b>
<b>    // the events are passed trough the kernel, and a mean output</b>
<b>    // error is computed.</b>
    
    if(fNValidEvents&lt;1) return 0.;
    
<b>    // we will now pass all the validation events through the kernel, and</b>
<b>    // compute the mean error on output </b>
    <a href="../ListOfTypes.html#Double_t">Double_t</a> error=0.;
    for (<a href="../ListOfTypes.html#Int_t">Int_t</a> j=0;j&lt;fNValidEvents;j++)
    {
	fValidTree-&gt;GetEvent(GetInputAdr(),GetTeachAdr(),j);
	error+=GoThrough(); // forward propagation and error on one event	
    }
    error=error/(<a href="../ListOfTypes.html#Double_t">Double_t</a>)fNValidEvents; // mean
    return error;
}
*/
/*
<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:TrainNCycles">TNNKernel::TrainNCycles</a>(TNNControlE *conte, <a href="../ListOfTypes.html#Int_t">Int_t</a> period, <a href="../ListOfTypes.html#Int_t">Int_t</a> nCycles)
{
<b>    // method to train on N cycles, with mixing and plot of errors</b>
<b>    // on the controller conte.</b>
    
    if(!conte){printf("no controller !n");return;}
    <a href="../ListOfTypes.html#Float_t">Float_t</a> errt,errv;
    for(<a href="../ListOfTypes.html#Int_t">Int_t</a> i=0;i&lt;nCycles;i++)
    {
	Mix();
	errt=(<a href="../ListOfTypes.html#Float_t">Float_t</a>)TrainOneCycle();
	errv=(<a href="../ListOfTypes.html#Float_t">Float_t</a>)Valid();
	printf("cycle %3i &gt; train : %7.3f",fNTrainCycles,errt);
	if(fNValidEvents)printf(" and valid : %7.3f n",errv);else printf("n");
	if(!(i%period)||i==(nCycles-1))
	{  
	    conte-&gt;AddTP(fNTrainCycles,errt); // add Train Point
	    conte-&gt;AddVP(fNTrainCycles,errv); // add Valid Point
	    conte-&gt;UpdateG();  // update graphics
	}     
	
    }
    
}
*/
<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:Export">TNNKernel::Export</a>(<a href="../ListOfTypes.html#Text_t">Text_t</a> *fileName)
{
<b>    // Put the structure in a file</b>
<b>    // WARNING : the weights and biases are stored with 4 digits</b>
<b>    // in decimal part.    </b>
<b>    // Learning parameters are not stored</b>
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c;
    
    if(!fW){printf("no unit !n");return;} 
    
    FILE *file;
    file=fopen(fileName,"w");
    
    fprintf(file,"%3in",fNHiddL);
    for(i=0;i&lt;fNHiddL+2;i++)fprintf(file,"%3in",fNUnits[i]);
    
    for(i=0;i&lt;fNHiddL+2;i++)
	for(l=0;l&lt;fNUnits[i];l++)fprintf(file,"%8.4fn",fBiases[i][l]);
	
    for(i=0;i&lt;fNHiddL+1;i++)
	for(l=0;l&lt;fNUnits[i];l++)
	    for(c=0;c&lt;fNUnits[i+1];c++)fprintf(file,"%8.4fn",fW[i][l][c]);
		
    fprintf(file,"%5in",fNTrainCycles);  
    fprintf(file,"%2.0fn",fUseBiases); 
    
    fclose(file);   
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:Import">TNNKernel::Import</a>(<a href="../ListOfTypes.html#Text_t">Text_t</a> *fileName)
{
<b>    // Get the structure from a file</b>
<b>    // WARNING : the weights and biases are stored with 4 digits</b>
<b>    // in decimal part.</b>
<b>    // Learning parameteres are not stored.  </b>
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,l,c,newI,newHL,newO;
    <a href="../ListOfTypes.html#Text_t">Text_t</a> hidden[100],piece[5];
    FILE *file;
    file=fopen(fileName,"r");
    if (file==0) {
	cerr &lt;&lt; "<a href=".././TNNKernel.html#TNNKernel:Import">TNNKernel::Import</a>: Could not open " &lt;&lt; fileName &lt;&lt; endl;
	return;
    }
    
    fscanf(file,"%3i",&amp;newHL);
    fscanf(file,"%3i",&amp;newI); 
    strcpy(hidden,"");
    for(i=1;i&lt;newHL;i++)
    {fscanf(file,"%s",piece);strcat(hidden,piece);strcat(hidden,":");} 
    fscanf(file,"%s",piece);strcat(hidden,piece);
    fscanf(file,"%3i",&amp;newO); 
    
    printf("New NN set to : %3i  %s  %3i n",newI,hidden,newO);
    FreeVW();			  
    AllocateVW(newI,hidden,newO);
    <a href="../ListOfTypes.html#Float_t">Float_t</a> tmpfl;
    for(i=0;i&lt;fNHiddL+2;i++)
	for(l=0;l&lt;fNUnits[i];l++){fDB[i][l]=0.;fscanf(file,"%f",&amp;tmpfl);*(fBiases[i]+l)=(<a href="../ListOfTypes.html#Double_t">Double_t</a>)tmpfl;}
	
    for(i=0;i&lt;fNHiddL+1;i++)
	for(l=0;l&lt;fNUnits[i];l++)
	    for(c=0;c&lt;fNUnits[i+1];c++){fDW[i][l][c]=0.;fscanf(file,"%f",&amp;tmpfl);*(fW[i][l]+c)=(<a href="../ListOfTypes.html#Double_t">Double_t</a>)tmpfl;}
		
		
    fscanf(file,"%5i",&amp;fNTrainCycles);  
    fscanf(file,"%f",&amp;tmpfl);fUseBiases=(<a href="../ListOfTypes.html#Double_t">Double_t</a>)tmpfl;  
    
    fclose(file);   
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:Mix">TNNKernel::Mix</a>()
{
<b>    // mix the events before learning. VERY IMPORTANT.</b>
<b>    // is has to be used before  TrainOneCycle() , </b>
<b>    // IT IS NOT used by TrainOneCycle() , you have to do the call yourself</b>
    
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i,i1,i2;
    <a href="../ListOfTypes.html#Int_t">Int_t</a> temp;
    for (i=0;i&lt;3*fNTrainEvents;i++)
    {
	i1=(<a href="../ListOfTypes.html#Int_t">Int_t</a>)(fRandom.Rndm()*(<a href="../ListOfTypes.html#Float_t">Float_t</a>)fNTrainEvents);
	i2=(<a href="../ListOfTypes.html#Int_t">Int_t</a>)(fRandom.Rndm()*(<a href="../ListOfTypes.html#Float_t">Float_t</a>)fNTrainEvents);
	temp=fEventsList[i1];
	fEventsList[i1]=fEventsList[i2];
	fEventsList[i2]=temp;
    }
    
<b>    //  for (i=0;i&lt;fNTrainEvents;i++)printf("%i n",fEventsList[i]);  </b>
<b>    //  printf("Mixed ... ");</b>
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetArraySize">TNNKernel::SetArraySize</a>(<a href="../ListOfTypes.html#Int_t">Int_t</a> size)
{
    DeleteArray();
    if (fEventsList) delete [] fEventsList;
    if(!size)return;
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i;
    fNTrainEvents=size;  
    fArrayIn  = new <a href="../ListOfTypes.html#Float_t">Float_t</a>*[fNTrainEvents];
    for (i=0;i&lt;fNTrainEvents;i++) fArrayIn[i] = new <a href="../ListOfTypes.html#Float_t">Float_t</a>[fNUnits[0]];
    
    fArrayOut = new <a href="../ListOfTypes.html#Float_t">Float_t</a>*[fNTrainEvents];  
    for (i=0;i&lt;fNTrainEvents;i++) fArrayOut[i] = new <a href="../ListOfTypes.html#Float_t">Float_t</a>[fNUnits[fNHiddL+1]];
    
    fEventsList = new <a href="../ListOfTypes.html#Int_t">Int_t</a>[fNTrainEvents];
    for (i=0;i&lt;fNTrainEvents;i++)fEventsList[i]=i;
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:DeleteArray">TNNKernel::DeleteArray</a>()
{
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i; 
    
    if(fArrayIn) 
    {
	for (i=0;i&lt;fNTrainEvents;i++)delete [] fArrayIn[i];
	delete [] fArrayIn;
	fArrayIn=0;
    }
    
    if(fArrayOut) 
    {
	for (i=0;i&lt;fNTrainEvents;i++)delete [] fArrayOut[i];
	delete [] fArrayOut;
	fArrayOut=0;
    }
    
}
/*
<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetTrainTree">TNNKernel::SetTrainTree</a>(TNNTree *t)
{
<b>    // method to associate a TNNTree to the kernel :</b>
<b>    // the events of the tree will be transferred in the internal</b>
<b>    // array of the kernel.</b>
    
    if(!t){printf("no tree !n");return;}
    <a href="../ListOfTypes.html#Int_t">Int_t</a> i;
    
<b>    //allocation  </b>
    
    SetArraySize((<a href="../ListOfTypes.html#Int_t">Int_t</a>)(t-&gt;GetTree()-&gt;GetEntries()));
    printf(" nbr evts for training : %i n",GetNTrainEvents());  
    
<b>    // loop  </b>
<b>    // the methods GetInputAdr() and GetTeachAdr()</b>
<b>    // return the adresses of arrays in kernel, and the method</b>
<b>    // GetEvent fills these adresses with event i of the train tree t</b>
<b>    // the method Fill(i) translates the filled arrays in the internal array</b>
    
    for (i=0;i&lt;(<a href="../ListOfTypes.html#Int_t">Int_t</a>)(t-&gt;GetTree()-&gt;GetEntries());i++)
    {
	t-&gt;GetEvent(GetInputAdr(),GetTeachAdr(),i);
	Fill(i);  
    }
    
}

<a href="../ListOfTypes.html#void">void</a> <a href=".././TNNKernel.html#TNNKernel:SetValidTree">TNNKernel::SetValidTree</a>(TNNTree *t)
{
<b>    // method to associate a TNNTree to the kernel :</b>
<b>    // a link will be done between the tree and the kernel.</b>
<b>    // it is not necessary to keep these events in the kernel</b>
    
    if(!t){printf("no tree !n");return;}
    fValidTree=t;
    fNValidEvents=(<a href="../ListOfTypes.html#Int_t">Int_t</a>)(t-&gt;GetTree()-&gt;GetEntries());
}
</pre>

<!--SIGNATURE-->
<br>
<hr>
<center>
<address>
<a href="http://root.cern.ch/root/Welcome.html">ROOT page</a> - <a href="../ClassIndex.html">Class index</a> - <a href="#TopOfPage">Top of the page</a><br>
</address>
</center>
<hr>
<address>
This page has been automatically generated. If you have any comments or suggestions about the page layout send a mail to <a href="mailto:rootdev@root.cern.ch">ROOT support</a>, or contact <a href="mailto:rootdev@root.cern.ch">the developers</a> with any questions or problems regarding ROOT.
</address>
</body>
</html>
